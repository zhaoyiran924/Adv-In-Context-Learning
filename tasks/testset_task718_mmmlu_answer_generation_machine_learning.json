{"Contributors": ["Frank Zhao"], "Source": ["measuring_massive_multitask_language_understanding"], "URL": ["https://github.com/hendrycks/test"], "Categories": ["Question Answering"], "Reasoning": ["Logical Reasoning"], "Definition": ["The following are multiple choice questions (with answers) about  machine learning.\n\n"], "Input_language": ["English"], "Output_language": ["English"], "Instruction_language": ["English"], "Domains": ["Computer Science -> Machine Learning"], "Positive Examples": [{"input": "A 6-sided die is rolled 15 times and the results are: side 1 comes up 0 times; side 2: 1 time; side 3: 2 times; side 4: 3 times; side 5: 4 times; side 6: 5 times. Based on these results, what is the probability of side 3 coming up when using Add-1 Smoothing?\n(A)2.0/15 (B)1.0/7 (C)3.0/16 (D)1.0/5 ", "output": "B"}, {"input": "Which image data augmentation is most common for natural images?\n(A)random crop and horizontal flip (B)random crop and vertical flip (C)posterization (D)dithering ", "output": "A"}, {"input": "You are reviewing papers for the World\u2019s Fanciest Machine Learning Conference, and you see submissions with the following claims. Which ones would you consider accepting? \n(A)My method achieves a training error lower than all previous methods! (B)My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise test error.) (C)My method achieves a test error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise cross-validaton error.) (D)My method achieves a cross-validation error lower than all previous methods! (Footnote: When regularisation parameter \u03bb is chosen so as to minimise cross-validaton error.) ", "output": "C"}, {"input": "To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\n(A)around 10 examples (B)around 100 examples (C)between 100 and 500 examples (D)more than 1000 examples ", "output": "D"}, {"input": "Traditionally, when we have a real-valued input attribute during decision-tree learning we consider a binary split according to whether the attribute is above or below some threshold. Pat suggests that instead we should just have a multiway split with one branch for each of the distinct values of the attribute. From the list below choose the single biggest problem with Pat\u2019s suggestion:\n(A)It is too computationally expensive. (B)It would probably result in a decision tree that scores badly on the training set and a testset. (C)It would probably result in a decision tree that scores well on the training set but badly on a testset. (D)It would probably result in a decision tree that scores well on a testset but badly on a training set. ", "output": "C"}], "Negative Examples": [{"input": "Statement 1| Linear regression estimator has the smallest variance among all unbiased estimators. Statement 2| The coefficients \u03b1 assigned to the classifiers assembled by AdaBoost are always non-negative.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": "I dont know.", "explanation": "Do not generate anything else apart from one of the following characters: 'A', 'B, 'C', 'D'."}, {"input": "Statement 1| RoBERTa pretrains on a corpus that is approximate 10x larger than the corpus BERT pretrained on. Statement 2| ResNeXts in 2018 usually used tanh activation functions.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": "B,C", "explanation": "Only one of the following options ('A', 'B', 'C', 'D') is allowed to be the answer. Multiple options can not be correct. The correct answer here would have been 'C'."}, {"input": "Suppose we like to calculate P(H|E, F) and we have no conditional independence information. Which of the following sets of numbers are sufficient for the calculation?\n(A)P(E, F), P(H), P(E|H), P(F|H) (B)P(E, F), P(H), P(E, F|H) (C)P(H), P(E|H), P(F|H) (D)P(E, F), P(E|H), P(F|H)", "output": "D", "explanation": "Without the conditional independence information we can use Bayes' theorem as P(H|E, F) * P(E, F) = P(E, F|H) * P(H). Hence we only need P(E, F), P(H), P(E, F|H) to calculate P(H|E, F). Hence the correct answer should be 'B'."}], "Instances": [{"id": "task718-a4cafa9fbb9c4f6aa00ecbdc28fda7d8", "input": "Compared to the variance of the Maximum Likelihood Estimate (MLE), the variance of the Maximum A Posteriori (MAP) estimate is ________\n(A)higher (B)same (C)lower (D)it could be any of the above", "output": ["C"]}, {"id": "task718-bc9a8a2d107241c5963a85515af7880f", "input": "What is the dimensionality of the null space of the following matrix? A = [[3, 2, \u22129], [\u22126, \u22124, 18], [12, 8, \u221236]]\n(A)0 (B)1 (C)2 (D)3", "output": ["C"]}, {"id": "task718-77543931849f4ebbaec7b2e4ca069630", "input": "Statement 1| The ID3 algorithm is guaranteed to find the optimal decision tree. Statement 2| Consider a continuous probability distribution with density f() that is nonzero everywhere. The probability of a value x is equal to f(x).\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["B"]}, {"id": "task718-cb795acf7a134085a353a3dcf7a09e9d", "input": "Statement 1| After mapped into feature space Q through a radial basis kernel function, 1-NN using unweighted Euclidean distance may be able to achieve better classification performance than in original space (though we can\u2019t guarantee this). Statement 2| The VC dimension of a Perceptron is smaller than the VC dimension of a simple linear SVM.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["B"]}, {"id": "task718-4157667f952649caaa7c1d39409764c8", "input": "Which of the following is/are true regarding an SVM?\n(A)For two dimensional data points, the separating hyperplane learnt by a linear SVM will be a straight line. (B)In theory, a Gaussian kernel SVM cannot model any complex separating hyperplane. (C)For every kernel function used in a SVM, one can obtain an equivalent closed form basis expansion. (D)Overfitting in an SVM is not a function of number of support vectors.", "output": ["A"]}, {"id": "task718-465401baa8254d29b43ccc884ac67956", "input": "Suppose you are given an EM algorithm that finds maximum likelihood estimates for a model with latent variables. You are asked to modify the algorithm so that it finds MAP estimates instead. Which step or steps do you need to modify?\n(A)Expectation (B)Maximization (C)No modification necessary (D)Both", "output": ["B"]}, {"id": "task718-8264e0a19d284ada99293c5b9473bd3b", "input": "Which of the following is a clustering algorithm in machine learning?\n(A)Expectation Maximization (B)CART (C)Gaussian Na\u00efve Bayes (D)Apriori", "output": ["A"]}, {"id": "task718-9c503113e0f048a7a0069a6a32f917be", "input": "Statement 1| Highway networks were introduced after ResNets and eschew max pooling in favor of convolutions. Statement 2| DenseNets usually cost more memory than ResNets.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["D"]}, {"id": "task718-3a7193fb5ae74cb4915e76df699d379e", "input": "Statement 1| The log-likelihood of the data will always increase through successive iterations of the expectation maximation algorithm. Statement 2| One disadvantage of Q-learning is that it can only be used when the learner has prior knowledge of how its actions affect its environment.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["B"]}, {"id": "task718-56b60d528b1145a1ba7ada6ddaf8833a", "input": "Statement 1| The kernel density estimator is equivalent to performing kernel regression with the value Yi = 1/n at each point Xi in the original data set. Statement 2| The depth of a learned decision tree can be larger than the number of training examples used to create the tree.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["B"]}, {"id": "task718-848f69e738a147a787ba0579a08c304a", "input": "Statement 1| RELUs are not monotonic, but sigmoids are monotonic. Statement 2| Neural networks trained with gradient descent with high probability converge to the global optimum.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["D"]}, {"id": "task718-432befa6064248ff91a2e99923684f55", "input": "The model obtained by applying linear regression on the identified subset of features may differ from the model obtained at the end of the process of identifying the subset during\n(A)Best-subset selection (B)Forward stepwise selection (C)Forward stage wise selection (D)All of the above", "output": ["C"]}, {"id": "task718-edcaa0e56e1e4879bd5dd01aaee61c99", "input": "Which of the following is false?\n(A)Semantic segmentation models predict the class of each pixel, while multiclass image classifiers predict the class of entire image. (B)A bounding box with an IoU (intersection over union) equal to $96\\%$ would likely be considered at true positive. (C)When a predicted bounding box does not correspond to any object in the scene, it is considered a false positive. (D)A bounding box with an IoU (intersection over union) equal to $3\\%$ would likely be considered at false negative.", "output": ["D"]}, {"id": "task718-16a3b03b310549329e7a7abe7a3238d2", "input": "Which of the following is false?\n(A)The following fully connected network without activation functions is linear: $g_3(g_2(g_1(x)))$, where $g_i(x) = W_i x$ and $W_i$ are matrices. (B)Leaky ReLU $\\max\\{0.01x,x\\}$ is convex. (C)A combination of ReLUs such as $ReLU(x) - ReLU(x-1)$ is convex. (D)The loss $\\log \\sigma(x)= -\\log(1+e^{-x})$ is concave", "output": ["C"]}, {"id": "task718-2ef620c61680429a84da143b489403a8", "input": "Statement 1| A neural network's convergence depends on the learning rate. Statement 2| Dropout multiplies randomly chosen activation values by zero.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["A"]}, {"id": "task718-d364c2aba69f4d589f70f0aa6c7e73bb", "input": "As of 2020, which architecture is best for classifying high-resolution images?\n(A)convolutional networks (B)graph networks (C)fully connected networks (D)RBF networks", "output": ["A"]}, {"id": "task718-0a94fb4fbfec4a1fa8f6b51a40bafbb9", "input": "Statement 1| Industrial-scale neural networks are normally trained on CPUs, not GPUs. Statement 2| The ResNet-50 model has over 1 billion parameters.\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["B"]}, {"id": "task718-6d6f61cdff6d437c80132c2b31f6d8c6", "input": "Which of the following is true of a convolution kernel?\n(A)Convolving an image with $\\begin{bmatrix}1 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ would not change the image (B)Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image (C)Convolving an image with $\\begin{bmatrix}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$ would not change the image (D)Convolving an image with $\\begin{bmatrix}0 & 0 & 0\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$ would not change the image", "output": ["B"]}, {"id": "task718-7003ff71314846eba6f9bc2c1eeda025", "input": "Statement 1| The SVM learning algorithm is guaranteed to find the globally optimal hypothesis with respect to its object function. Statement 2| After being mapped into feature space Q through a radial basis kernel function, a Perceptron may be able to achieve better classification performance than in its original space (though we can\u2019t guarantee this).\n(A)True, True (B)False, False (C)True, False (D)False, True", "output": ["A"]}, {"id": "task718-8c00f26e343c41dba354868cffb0479b", "input": "A and B are two events. If P(A, B) decreases while P(A) increases, which of the following is true?\n(A)P(A|B) decreases (B)P(B|A) decreases (C)P(B) decreases (D)All of above", "output": ["B"]}], "Instance License": ["MIT"]}